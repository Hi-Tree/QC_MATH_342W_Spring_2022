---
title: "Practice Lecture 5 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "February 10, 2021"
---


## Libraries in R

So far we've only made use of "base R". This is the funcionality included from a vanilla installation of R. 

R has a huge worldwide community of contributors. Some contribute to newer version of base R, but most create open-source "R packages" or "libraries". Many libraries come preinstalled. For instance, the MASS library which stands for "Modern Applied Statistics with S" (a famous textbook of R). We can call a function from the MASS library via the following:

```{r}
MASS::as.fractions(0.99)
MASS::as.fractions(pi)
```

Note we cannot just

```{r}
as.fractions(pi)
```

We made use of the scope operator "::" to access a namespace beyond the usual "global namespace" which we've been used to. Parenthetically, you can use the ":::" to access the private / internal functions and variables. Anyone who understands object-oriented programming with public interfaces / APIs would cringe at this!!!

If we are using the MASS library a lot, using the scope operator may get annoying. So similar to the "with" command, we can call

```{r}
library(MASS)
```

which loads all public methods (aka "exported" functions) into the public namespace. 

Now, after the library invocation we can do the following and treat it as a normal function:

```{r}
as.fractions(pi)
```

Is this always a good idea? No... everytime you call `library` it "dirties" the namespace by putting all the functions there and rewriting over functions there previously. This is bad because you are more likely to get namespace conflicts. For instance. Let's say package `kapelner` had a weird `sample` function. This would be clear:

```{r}
v = rnorm(100)
kapelner::sample(v)
sample(v)
```

The first line is doing the special sample function and the second is using base R's sample. But if I do this:

```{r}
library(kapelner)
#...
#...
###10,000 lines of code in which you forget about the fact that the kapelner library is loaded
#...
#...
sample(v)
```

You may think you're getting base R sample function, but you're not and now you're in  bug-city! You would have to do the following to be explicit:

```{r}
library(kapelner)
sample(v)
base::sample(v)
```

This is not a recommended thing to do. It's also not recommended for package developers to name functions the same as common base R functions. But this doesn't stop them!

Back to real packages... the content for the MASS package was sitting on the hard drive since it comes with R. But what if you want to use a package that does not come with R? We'll have to install the package just like pip for Python, Rubygems for Ruby, R has a package management system built in. For example, here's a useful package for time series / finance stuff:

```{r}
install.packages("tseries")
```

Note that it knew where to go online - it went to a CRAN mirror. CRAN is the official repository for R packages. Now that it's installed, step 2 is to load it into namespace so we can more seamlessly access its functionality.

```{r}
library(tseries)
```

What was a welcome message.

This library is really cool e.g.

```{r}
ibm_stock_history = get.hist.quote(instrument = "IBM", start = "2018-01-01", end = "2018-02-01")
ibm_stock_history
```

Is this a data frame?

```{r}
class(ibm_stock_history)
```

Nope - they made their own data type. If we had a unit on "writing your own R packages", I would explain how this is done but alas there is no time...

Let's say you're sharing your code with someone and one of your lines is loading a library e.g.

```{r}
library(a_library_my_computer_does_not_have_installed_yet)
```

And my computer doesn't have this library. Then we need to stop what we're doing and install. This could be annoying. Here is a convenience: use the pacman package that installs if necessary:

```{r}
if (!require("pacman")){install.packages("pacman")} #installs pacman if necessary but does not load it!
pacman::p_load(devtools) #ensures that devtools gets installed and loaded into the workspace but pacman does not (very tidy)!
```

It is typical to then have a few lines declaring all packages on top of your R/Rmd script file. Here is an example header from one of my projects. 

```{r}
#if (!require("pacman")){install.packages("pacman")}
#pacman::p_load(knitr, randomForest, dplyr, tidyverse, doParallel, xtable, pracma, yaml)
```

We will be seeing this in pretty much all our demos in the future. It is very rare to be coding in R without making use of packages beyond base R. I'm going to require the use of pacman for HW / projects, etc. Just makes code easier to share, run, etc.

The devtools package is important for modern R usage. It allows downloading R packages directly from source that are not even on CRAN. This allows you to get "bleeding edge" features. For example:

```{r}
install_github("yihui/knitr")
```

However this doesn't always work!

```{r}
install_github("hadley/ggplot2")
```

Why can this fail? Because the computer you're running this on is not setup for compiling C++. Admittedly, MAC's usually succeed here and Windows usually fails here. To make it succeed you need to install a separate program beyond R called Rtools. This is one of the big advantages of using Linux and MAC over Windows - Windows just is more buggy when it comes to "real coding" and it gets in the way when you're out there trying to get stuff done. Linux absolutely rules here and because Linux is usually the production environment anyway, it may make sense to use it for all your assignments and coding anyway.

Note, you can use the pacman library for this type of installation too. So your header becomes:

```{r}
if (!require("pacman")){install.packages("pacman")}
pacman::p_load(devtools)
pacman::p_load_gh("hadley/ggplot2")
```

# Convenient Mapping Function for Lists with the purrr package

We first load the library.

```{r}
pacman::p_load(purrr)
```

We will see later that the library `purrr` is part of a collection of libraries called the `tidyverse`.

Now imagine you have a collection of objects in a list. For example, let's let the object be matrices with different sizes:

```{r}
my_matrix_list = list()
my_matrix_list[["first"]] = matrix(rnorm(9), nrow = 3)
my_matrix_list[["second"]] = matrix(rnorm(12), nrow = 2)
my_matrix_list[["third"]] = matrix(rnorm(8), nrow = 4)
my_matrix_list
```

And you want to operate on each of those objects and return a list. Let's say I want to get back the dimensions, or the first rows, or the average values and return the same keys:

```{r}
my_dims_list = modify(my_matrix_list, ~ dim(.x))
my_dims_list
my_first_rows_list = modify(my_matrix_list, ~ .x[1, ])
my_first_rows_list
my_avgs_list = modify(my_matrix_list, ~ mean(.x))
my_avgs_list
```

This is a very convenient function known as "mapping" in functional programming. It saves a few lines of code e.g. the first `modify` would be:

```{r}
my_dims_list = list() #make new list to store keys --> dimensions of original matrices
for (key in names(my_matrix_list)){ #iterate over all list by key
  .x = my_matrix_list[[key]] #get value at the key for this iteration
  my_dims_list[[key]] = dim(.x) #run function on value and save it to new list
}
my_dims_list
```

The above which takes 5 lines and is repeated again and again and again in code all takes one line using the `modify` function. 

The `modify` function uses funky syntax which is not standard R. And it doesn't have to be; packages are allowed to extend the language and use symbols to create their own little mini-language. The `.x` above is a dummy variable for the value in the iteration in the imagined for loop (like in my rewritten boilerplate code above). The "~" tilde symbol we will be seeing in base R later on in class but in a completely different context. Here it just means "run the following function".

Modify is just one of the functions in the `purrr` package. See the following cheatsheet for more convenient functions: https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf.


## Loading datasets from R packages

Since R is a language built for data and statistics, it has a ton of interesting data sets by default and even more that are contained in packages. There is really just one command to know:

```{r}
rm(list = ls())
data(iris) #load the iris dataset (as a data frame). This dataset is included in the package "datasets" which is autoloaded by default
class(iris)
?iris
#3 things I always do immediately upon getting a dataset
head(iris)
str(iris)
summary(iris)
```

Here is another very famous dataset

```{r}
MASS::Boston #this just references the object but does not load it into the environment
data(Boston) #error since package MASS is not loaded by default
data(Boston, package = "MASS") #package argument not needed if package loaded 
head(Boston)
```

Most data sets are names some descriptive name like "loandata" or "cars". R has so many datasets. Here they all are by package installed:

```{r}
data(package = .packages(all.available = TRUE))
```

## Continue discussion concerning data frames and the modeling from class

We quickly recreate our data frame from last class:

```{r}
n = 100
X = data.frame(
  salary = round(rnorm(n, 50000, 20000)),
  has_past_unpaid_loan = rbinom(n, size = 1, prob = 0.2),
  past_crime_severity = sample(
    c("no crime", "infraction", "misdimeanor", "felony"),
    size = n,
    replace = TRUE,
    prob = c(.50, .40, .08, .02)
  )
)
row.names(X) = c(
  "Sophia", "Emma", "Olivia", "Ava", "Mia", "Isabella", "Riley", 
  "Aria", "Zoe", "Charlotte", "Lily", "Layla", "Amelia", "Emily", 
  "Madelyn", "Aubrey", "Adalyn", "Madison", "Chloe", "Harper", 
  "Abigail", "Aaliyah", "Avery", "Evelyn", "Kaylee", "Ella", "Ellie", 
  "Scarlett", "Arianna", "Hailey", "Nora", "Addison", "Brooklyn", 
  "Hannah", "Mila", "Leah", "Elizabeth", "Sarah", "Eliana", "Mackenzie", 
  "Peyton", "Maria", "Grace", "Adeline", "Elena", "Anna", "Victoria", 
  "Camilla", "Lillian", "Natalie", "Jackson", "Aiden", "Lucas", 
  "Liam", "Noah", "Ethan", "Mason", "Caden", "Oliver", "Elijah", 
  "Grayson", "Jacob", "Michael", "Benjamin", "Carter", "James", 
  "Jayden", "Logan", "Alexander", "Caleb", "Ryan", "Luke", "Daniel", 
  "Jack", "William", "Owen", "Gabriel", "Matthew", "Connor", "Jayce", 
  "Isaac", "Sebastian", "Henry", "Muhammad", "Cameron", "Wyatt", 
  "Dylan", "Nathan", "Nicholas", "Julian", "Eli", "Levi", "Isaiah", 
  "Landon", "David", "Christian", "Andrew", "Brayden", "John", 
  "Lincoln"
)
X
```

Remember our cross tab? Now we can get fancier using our new libary skills. Any Stata fans out there?

```{r}
pacman::p_load(gmodels)
CrossTable(X$has_past_unpaid_loan, X$past_crime_severity, chisq = TRUE)
```


And add a new variable, the response to the data frame:

```{r}
X$paid_back_loan = factor(rbinom(n, size = 1, prob = 0.9), labels = c("No", "Yes"))
```

Note that our matrix is now no longer just $X$; it includes $y$. I could make a renamed copy, but I want to show off dropping this column and create a new object that's both features and response column-binded together:

```{r}
y = X$paid_back_loan
X$paid_back_loan = NULL #drop column
Xy = cbind(X, y) #an aside: what do you think the "rbind" function does?
head(Xy) #make sure that worked
summary(Xy) #much better now!
#Note: Xy = X; rm(X) would've been easier
```

I prefer calling the full training set ${X, y}$ a data frame called $Xy$. 


The object $X$ is now extraneous, so we should clean up our workspace now.

```{r}
rm(list = setdiff(ls(), "Xy"))
```


## The Null Model

```{r}
#There's no standard R function for sample mode!!!
sample_mode = function(data){
  mode_name = names(sort(-table(data)))[1]
  if (class(data) == "factor"){
    factor(mode_name, levels = levels(data))
  } else if (class(data) == "numeric"){
    as.numeric(mode_name)
  } else if (class(data) == "integer"){
    as.integer(mode_name)
  } else {
    mode_name
  }
}

g0 = function(){
  sample_mode(Xy$y) #return mode regardless of x
} 

g0()
```


## The Threshold Model

Let's compute the threshold model and see what happens. Here's an inefficent but quite pedagogical way to do this:

```{r}
n = nrow(Xy)
num_errors_by_parameter = matrix(NA, nrow = n, ncol = 2)
colnames(num_errors_by_parameter) = c("threshold_param", "num_errors")
y_logical = Xy$y == "Yes"
for (i in 1 : n){
  threshold = Xy$salary[i]
  num_errors = sum((Xy$salary > threshold) != y_logical)
  num_errors_by_parameter[i, ] = c(threshold, num_errors)
}
num_errors_by_parameter

#look at all thresholds in order
num_errors_by_parameter[order(num_errors_by_parameter[, "num_errors"]), ]

#now grab the smallest num errors
best_row = order(num_errors_by_parameter[, "num_errors"])[1]
x_star = c(num_errors_by_parameter[best_row, "threshold_param"], use.names = FALSE)
x_star
```

Let's program `g`, the model that is shipped as the prediction function for future `x_*`

```{r}
g = function(x){
  ifelse(x > x_star, 1, 0)
} 

g(15000)
```


## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
rm(list = ls())
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 10] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

First question. Let $\mathcal{H}$ be the set $\{0, 1\}$ meaning $g = 0$ or $g = 1$. What are the error rates then on $\mathbb{D}$? 

```{r}
#If always 0, all the 1's are errors
239 / (444 + 239)
#If always 1, all the 0's are errors
444 / (444 + 239)

g0 = function(){
  sample_mode(y_binary) #return mode regardless of x's
} 

g0()
```

If your $g$ can't beat that, either your features $x_1, \ldots, x_p$ are terrible, and/or $\mathcal{H}$ was a terrible choice and/or $\mathcal{A}$ can't pull its weight.

Okay... back to the "perceptron learning algorithm".

Let's do so for one dimension - just "V1" in the breast cancer data. You will do an example with more features for the lab.

```{r}
# y_binary = ifelse(y_binary == 1, 0, 1)
MAX_ITER = 1000
w_vec = rep(0, 2) #intialize a 2-dim vector

X1 = as.matrix(cbind(1, X[, 1, drop = FALSE]))

for (iter in 1 : MAX_ITER){  
  for (i in 1 : nrow(X1)){
    x_i = X1[i, ]
    yhat_i = ifelse(sum(x_i * w_vec) > 0, 1, 0)
    y_i = y_binary[i]
    w_vec[1] = w_vec[1] + (y_i - yhat_i) * x_i[1]
    w_vec[2] = w_vec[2] + (y_i - yhat_i) * x_i[2]
  }
}
w_vec
```

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Looks like the perceptron fit to just the first feature beat the null model (at least on the data in $\mathbb{D}$). Is this expected? Yes if the first feature is at all predictive of `y`.

